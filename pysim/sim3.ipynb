{
 "metadata": {
  "name": "",
  "signature": "sha256:cc16f8fff91f688f5369244be02db7fa0e5c950594438442ebb2a6af66f0b498"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "from pylab import *\n",
      "from numpy import *\n",
      "from numpy.random import randn\n",
      "from random import choice, random, gauss\n",
      "from collections import defaultdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class svm_model:\n",
      "    def __init__(self, lmbda, dim):\n",
      "        self.dim = dim\n",
      "        self.w = zeros(dim)\n",
      "        self.lmbda = lmbda\n",
      "\n",
      "    def copy(self):\n",
      "        m = svm_model(self.lmbda, self.dim)\n",
      "        m.w = self.w.copy()\n",
      "        return m\n",
      "\n",
      "    def set(self, other):\n",
      "        self.w = other.w.copy()\n",
      "        self.lmbda = other.lmbda\n",
      "        self.dim = other.dim\n",
      "\n",
      "    def randomize_model(self,  scale=1.0, density=1.0):\n",
      "        for i in range(0, self.dim):\n",
      "            self.w[i] = gaus(0, scale) if random() < density else 0.\n",
      "\n",
      "    def gradient(self, x, y):\n",
      "        prod = y * (self.w.dot(x))\n",
      "        return self.lmbda * self.w - ((y * x) if prod < 1 else 0)\n",
      "\n",
      "    def apply_gradient(self, grad, eta):\n",
      "        self.w -= eta * grad\n",
      "        return self.w\n",
      "\n",
      "    # def learn(self, x, y, t):\n",
      "    #     eta = 1.0/(self.lmbda*(t+1))\n",
      "    #     prod = y * (self.w.dot(x))\n",
      "    #     grad = self.lmbda * w - ((y * x) if prod < 1 else 0)\n",
      "    #     self.w -= eta*grad\n",
      "    #     return self.w\n",
      "\n",
      "    def predict(self, x):\n",
      "        sign(self.w.dot(x))\n",
      "\n",
      "    def point_loss(self, x, y):\n",
      "        return max(0., 1. - y*(self.w.dot(x)))\n",
      "\n",
      "    def data_loss(self, data):\n",
      "        loss = 0.\n",
      "        for (x, y) in data:\n",
      "            loss += max(0., 1. - y*(self.w.dot(x)))\n",
      "        return loss\n",
      "\n",
      "    def model_loss(self):\n",
      "        return pow(linalg.norm(self.w), 2)*self.lmbda/2\n",
      "\n",
      "    def loss(self, data):\n",
      "        return self.data_loss(data) + self.model_loss()\n",
      "\n",
      "    def accuracy(self, data):\n",
      "        correct = 0\n",
      "        for (x,y) in data:\n",
      "            correct += 1 if y * self.w.dot(x) >= 0 else 0\n",
      "        return double(correct) / len(data)\n",
      "\n",
      "    def average_models(self, models):\n",
      "        self.lbmda = models[0].lmbda\n",
      "        self.dim = len(models[0])\n",
      "        new_w = zeros(dim)\n",
      "        for m in models:\n",
      "            new_w += m.w\n",
      "        self.w = new_w / double(len(models))\n",
      "        return self.w\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def serial_SGD(model, data, iterations, initial_t=0):\n",
      "    history = []\n",
      "    for t in range(initial_t, initial_t+iterations):\n",
      "        (x, y) = choice(data)\n",
      "        grad = model.gradient(x, y)\n",
      "        eta = 1.0/(model.lmbda*(t+1))\n",
      "        model.apply_gradient(grad, eta)\n",
      "        history.append(model.copy())\n",
      "    return history"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def batch_AVG_SGD(model, data_group, iterations, initial_t=0):\n",
      "    # Advance all the models serially\n",
      "    histories = []\n",
      "    local_models = []\n",
      "    for data in data_group:\n",
      "        local_model = model.copy()\n",
      "        history = serial_SGD(local_model, data, iterations, initial_t)\n",
      "        histories.append(history)\n",
      "        local_models.append(local_model)\n",
      "    model.average_models(local_models)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bsp_Hogwild(model_group, data_group, iterations, initial_t=0):\n",
      "    # Advance all the models serially\n",
      "    deltas = {}\n",
      "    for (model, data) in zip(model_group, data_group):\n",
      "        for t in range(initial_t, initial_t+iterations):\n",
      "            (x, y) = choice(data)\n",
      "            grad = model.gradient(x, y)\n",
      "            eta = 1.0/(model.lmbda*(t+1))\n",
      "            model.apply_gradient(grad, eta)\n",
      "            deltas\n",
      "            (x, y) = choice(data)\n",
      "        grad = model.gradient(x, y)\n",
      "        eta = 1.0/(model.lmbda*(t+1))\n",
      "        model.apply_gradient(grad, eta)\n",
      "\n",
      "        serial_SGD(model, data, iterations, initial_t)\n",
      "    # Compute the averaged model\n",
      "    model_group[0].average_models(model_group)\n",
      "    # set all the models to be the model average\n",
      "    for model in model_group:\n",
      "        model.set(model_group[0])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mini_batch_GD(model, data_group, iterations, initial_t=0):\n",
      "    grad_sum = None\n",
      "    total = 0\n",
      "    # For each group of data, compute the gradient average\n",
      "    for data in data_group:\n",
      "        for t in range(initial_t, initial_t + iterations):\n",
      "            (x, y) = choice(data)\n",
      "            if grad_sum is None:\n",
      "                grad_sum = model.gradient(x, y)\n",
      "            else:\n",
      "                grad_sum += model.gradient(x, y)\n",
      "            total += 1\n",
      "    eta = 1.0/(model.lmbda*(initial_t+1))\n",
      "    model.apply_gradient(grad_sum / double(total), eta)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Generators"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Datagen:\n",
      "    UNIFORM = 0\n",
      "    SKEWED = 1\n",
      "    SPLIT = 2\n",
      "    POINT_CLOUD = 3\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gen_synth_data(num_samples, dim, nprocs, proc, params):\n",
      "    noise = params['noise']\n",
      "    use_bias = params['bias']\n",
      "    if params['type'] == Datagen.POINT_CLOUD:\n",
      "        pluscloud = array(params['offset1'] * ones(dim))\n",
      "        negcloud = array(params['offset2'] * ones(dim))\n",
      "\n",
      "        cloud_var = params['cloud_var']\n",
      "        isplus = (proc % 2) == 0\n",
      "        cloud_center = pluscloud if isplus else negcloud\n",
      "        y = 1 if isplus else -1\n",
      "        data = []\n",
      "    \n",
      "        for i in range(0, num_samples):\n",
      "            # Generate X\n",
      "            x = cloud_center + cloud_var * randn(dim)\n",
      "            # Add bias term if necessary\n",
      "            if use_bias:\n",
      "                x = append(x, 1.)\n",
      "            # Generate\n",
      "            obsY = y if random() < noise else -y\n",
      "            data.append((x, obsY))\n",
      "        return data\n",
      "    if params['type'] == Datagen.SPLIT:\n",
      "        if use_bias:\n",
      "            dim += 1\n",
      "        true_w = randn(dim)\n",
      "        cloud_var = params['cloud_var']\n",
      "        data = []\n",
      "        for i in range(0, num_samples):\n",
      "            # Generate X\n",
      "            x = randn(dim)\n",
      "            # set the bias term\n",
      "            if hasBias:\n",
      "                x[-1] = 1.\n",
      "            y = 1 if w_train.dot(x) > 0 else -1\n",
      "            if random() < noise:\n",
      "                y = -y\n",
      "            data.append((x,y))\n",
      "        return data\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_data(data, model = None):\n",
      "    plot([x[0] for (x,y) in data if y == -1], [x[1] for (x,y) in data if y == -1], 'o', color=\"red\")\n",
      "    plot([x[0] for (x,y) in data if y == 1], [x[1] for (x,y) in data if y == 1], '+', color=\"blue\")\n",
      "    if model is not None:\n",
      "        plot([m[0] for m in MODEL_HIST[model]], [m[1] for m in MODEL_HIST[model]], 's-', color=\"black\")\n",
      "\n",
      "    show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Experiments"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Experimental setup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DATA_PARAMS = {\n",
      "    'type': Datagen.POINT_CLOUD,\n",
      "    'noise': 0.0,\n",
      "    'offset1': 5.,\n",
      "    'offset2': 10.,\n",
      "    'cloud_var': 1,\n",
      "    'bias': True\n",
      "    }\n",
      "NUM_SAMPLES  = 1000\n",
      "DIM = 2\n",
      "NUM_PROCS = 4\n",
      "LMBDA = 0.1\n",
      "PLOT_GRANULARITY = 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "proc_data = {}\n",
      "for proc in range(0, NUM_PROCS):\n",
      "    proc_data[proc] = gen_synth_data(NUM_SAMPLES, DIM, NUM_PROCS, proc, DATA_PARAMS)\n",
      "all_data = []\n",
      "for data in proc_data.values():\n",
      "    all_data += data\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_data(all_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = svm_model(LMBDA, DIM+1)\n",
      "hist = serial_SGD(model, all_data, NUM_SAMPLES * 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loss = [m.loss(all_data) for m in hist[0::PLOT_GRANULARITY]]\n",
      "loss_x = arange(0, len(hist), PLOT_GRANULARITY)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(loss_x, loss)\n",
      "xlabel(\"Iteration\")\n",
      "ylabel(\"Loss\")\n",
      "gca().set_yscale('log')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy = [m.accuracy(all_data) for m in hist[0::PLOT_GRANULARITY]]\n",
      "accuracy_x = arange(0, len(hist), PLOT_GRANULARITY)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(accuracy_x, accuracy)\n",
      "xlabel(\"Iteration\")\n",
      "ylabel(\"Loss\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\begin{align}\n",
      "\\min_x & \\sum_{i=1}^p f_i(x) \\\\\n",
      "\\min_{z_1, \\ldots, z_p} & \\sum_{i=1}^p f_i(z_i) \\\\\n",
      "\\text{ s.t. } & \\forall_i \\, z_i = \\frac{1}{p}\\sum_{j=1}^p z_j\n",
      "\\end{align}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$L(z_1, \\ldots, z_p, \\lambda_1, \\ldots, \\lambda_p) =\n",
      "\\sum_{i=1}^p f_i(z_i) + \\sum_{i=1}^p \\lambda_i (z_i - \\frac{1}{p}\\sum_{j=1}^p z_j)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "L(z_1, \\ldots, z_p, \\lambda_1, \\ldots, \\lambda_p) & =\n",
      "\\sum_{i=1}^p f_i(z_i) + \\sum_{i=1}^p \\lambda_i \\left(z_i - \\frac{1}{p}\\sum_{j=1}^p z_j \\right) \\\\\n",
      "z_i^{(t+1)} & = z_i^{(t)} - \\eta_t \\nabla_{z_i} L(z, \\lambda) \\\\\n",
      "z_i^{(t+1)} & = z_i^{(t)} - \\eta_t (\\nabla_{z_i} f_i(z_i) + \\lambda_i) \\\\\n",
      "\\lambda_i{(t+1)} & = \\lambda_i^{(t)} + \\alpha_t  \\nabla_{\\lambda_i} L(z, \\lambda) \\\\\n",
      "\\lambda_i{(t+1)} & = \\lambda_i^{(t)} + \\alpha_t \\left(z_i - \\frac{1}{p}\\sum_{j=1}^p z_j\\right)\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "\n",
      "$\\lambda_i$ is a vector with a small change using the previously calculated average less the local model ($avg-x_i$). We change how often we recompute $\\lambda_i$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
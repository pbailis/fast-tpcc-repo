{
 "metadata": {
  "name": "",
  "signature": "sha256:4957257cc614f93486225d6ed56a252741ec1dbe51f3a18b02d431d4635fc2a6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Skew Test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm, linear_model\n",
      "from csv import reader\n",
      "import random\n",
      "from numpy.linalg import norm\n",
      "from numpy import average, var\n",
      "from pylab import *\n",
      "import urllib\n",
      "from zipfile import ZipFile\n",
      "import pandas as pd\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Enable inline plotting"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Support functionality"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_dir = \"./data/\"\n",
      "if not os.path.exists(data_dir):\n",
      "    os.makedirs(data_dir)\n",
      "    \n",
      "# Download the URL into data_dir + result_dir\n",
      "def get_dataset(url, result_dir):\n",
      "    fname = url.split(\"/\")[-1]\n",
      "    if not os.path.isfile(data_dir + fname):\n",
      "        print \"Downloading \" + fname \n",
      "        data = urllib.urlretrieve(url, data_dir + fname)\n",
      "    if fname[-3:] == \"zip\":\n",
      "        ZipFile(data_dir + fname).extractall(data_dir + result_dir)\n",
      "\n",
      "def make_binary_df(data):\n",
      "    data2 = pd.DataFrame()\n",
      "    for col_name in data:\n",
      "        if data[col_name].dtype is dtype('O'):\n",
      "            # Remove whitespace in column\n",
      "            old_col = data[col_name].map(str.strip)\n",
      "            # determine the unique values\n",
      "            values = old_col.unique() # pd.unique(old_col.values)\n",
      "            if size(values) == 2:\n",
      "                # Ensure a canoncial ordering\n",
      "                values = sort(values)\n",
      "                # special case where we only need one column\n",
      "                new_col = 2.0 * (old_col == values[1]) - 1.0\n",
      "                data2[col_name] = new_col\n",
      "            else:\n",
      "                # create a new binary column for each value\n",
      "                for v in values:\n",
      "                    new_col = 2.0 * (old_col == v) - 1.0\n",
      "                    data2[col_name + \"_is_\" + v] = new_col\n",
      "        else:\n",
      "            data2[col_name] = data[col_name]\n",
      "    return data2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Bank Marketing Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Download and open the data file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_dataset(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip\", \"bank\")\n",
      "data = pd.read_csv(data_dir + \"bank/bank-full.csv\", sep = ';')\n",
      "data.tail(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Convert string fields into multiple binary indicator columns."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data2 = make_binary_df(data)\n",
      "data2.tail(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Split out the $X$ and $y$ values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = data2.drop(\"y\", axis=1).as_matrix()\n",
      "Y = data2['y'].as_matrix()\n",
      "n, p = X.shape\n",
      "print \"n: \" + str(n)\n",
      "print \"p: \" + str(p)\n",
      "assert(size(Y) == n) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Run Skew Experiments"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trials = 5\n",
      "cluster_sizes = np.array([2,4,8,16,32,64,128,256])\n",
      "test_prop = 0.2\n",
      "\n",
      "testSize = int(test_prop * n)\n",
      "indices = np.arange(n)\n",
      "\n",
      "globalModelScores = np.zeros(trials)\n",
      "globalModels = np.zeros((trials, p+1))\n",
      "localModelScores = np.zeros((trials, size(cluster_sizes)))\n",
      "meanModelScores = np.zeros((trials, size(cluster_sizes)))\n",
      "modelVariance = np.zeros((trials, size(cluster_sizes)))\n",
      "\n",
      "\n",
      "for t in range(0, trials):\n",
      "    print \"Starting trial: \" + str(t)\n",
      "    # Randomly create training and test split\n",
      "    shuffle(indices)\n",
      "    Xtest = X[indices[:testSize],:]\n",
      "    Ytest = Y[indices[:testSize]]\n",
      "    Xtrain = X[indices[testSize:],:]\n",
      "    Ytrain = Y[indices[testSize:]]\n",
      "\n",
      "    # Train a global model on all the data\n",
      "    globalModel = linear_model.LogisticRegression()\n",
      "    globalModel.fit(Xtrain, Ytrain)\n",
      "    globalModelScores[t] = globalModel.score(Xtest, Ytest)\n",
      "    globalModels[t,:] = np.append(globalModel.coef_.flatten(), globalModel.intercept_)\n",
      "    \n",
      "    # Train local models on each machine\n",
      "    for m in range(0, size(cluster_sizes)):\n",
      "        num_machines = cluster_sizes[m]\n",
      "        split_Xtrain = np.array_split(Xtrain, num_machines)\n",
      "        split_Ytrain = np.array_split(Ytrain, num_machines)\n",
      "        # Allocate arrays to store local model info\n",
      "        coeffs = np.zeros((num_machines, p+1))\n",
      "        scores = np.zeros(num_machines)\n",
      "        \n",
      "        # Train local models\n",
      "        for i in range(0, num_machines):\n",
      "            localModel = linear_model.LogisticRegression()\n",
      "            localModel.fit(split_Xtrain[i], split_Ytrain[i])\n",
      "            scores[i] = localModel.score(Xtest, Ytest)\n",
      "            coeffs[i, :] = np.append(localModel.coef_.flatten(), localModel.intercept_)\n",
      "        \n",
      "        # Compute cluster statistics\n",
      "        localModelScores[t,m] = np.mean(scores)\n",
      "        modelVariance[t,m] = np.mean(np.square(coeffs - globalModels[t,:]))\n",
      "        \n",
      "        # Super hack to try and estimate the score of the model obtained by averaging the models\n",
      "        avgCoeff = np.array([np.mean(coeffs, axis=0)])\n",
      "        meanModel = linear_model.LogisticRegression()\n",
      "        # Need to first train the model to set extra class variables\n",
      "        meanModel.fit(split_Xtrain[0], split_Ytrain[0])\n",
      "        # swap out the model parameters with mean parameters\n",
      "        meanModel.coef_ = avgCoeff[:, :-1]\n",
      "        meanModel.intercept_ = avgCoeff[:,-1]\n",
      "        meanModelScores[t,m] = meanModel.score(Xtest, Ytest)\n",
      "\n",
      "        \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Analyze the results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Global Bias: \" + str(np.mean((Y +2.0)/2.0))\n",
      "print \"Global Accuracy: \" + str( np.mean(globalModelScores))\n",
      "plot(cluster_sizes, np.mean(localModelScores, axis=0) / np.mean(globalModelScores))\n",
      "plot(cluster_sizes, np.mean(meanModelScores, axis=0) / np.mean(globalModelScores))\n",
      "legend([\"local\", \"Mean Model\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "plot(cluster_sizes, np.mean(modelVariance, axis=0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Scratch Material"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# globalSVM = svm.LinearSVC()\n",
      "# globalSVM.fit(Xtrain, Ytrain)\n",
      "# globalLogistic = linear_model.LogisticRegression(penalty='l1')\n",
      "# globalLogistic.fit(Xtrain, Ytrain)\n",
      "# print globalSVM.score(Xtest, Ytest)\n",
      "# print globalLogistic.score(Xtest, Ytest)\n",
      "\n",
      "# groups = data.groupby('job')\n",
      "# sz = groups.size()\n",
      "# print sz"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Adult Sensus Dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_dataset(\"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", \"adult\")\n",
      "get_dataset(\"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", \"adult\")\n",
      "adult_names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \n",
      "               \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
      "               \"hours-per-week\", \"native_country\", \">50k\"]\n",
      "train_data = pd.read_csv(data_dir + \"adult.data\", sep = ',', names=adult_names,  header=None).dropna()\n",
      "test_data = pd.read_csv(data_dir + \"adult.test\", sep = ',', names=adult_names,  header=None, skiprows=[0]).dropna()\n",
      "data = train_data.append(test_data)\n",
      "# Clean up last column which might have punctuation!?  \n",
      "data[\">50k\"] = data[\">50k\"].map(lambda x: x.replace(\".\", \"\"))\n",
      "print \"Data size: \" + str(data.shape)\n",
      "data.head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "binary_data = make_binary_df(data)\n",
      "print \"Data size: \" + str(binary_data.shape)\n",
      "binary_data.head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = binary_data.drop(\">50k\", axis=1).as_matrix()\n",
      "Y = binary_data[\">50k\"].as_matrix()\n",
      "n, p = X.shape\n",
      "print \"n: \" + str(n)\n",
      "print \"p: \" + str(p)\n",
      "assert(size(Y) == n) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trials = 5\n",
      "cluster_sizes = np.array([2,4,8,16,32,64,128,256])\n",
      "test_prop = 0.2\n",
      "\n",
      "testSize = int(test_prop * n)\n",
      "indices = np.arange(n)\n",
      "\n",
      "globalModelScores = np.zeros(trials)\n",
      "globalModels = np.zeros((trials, p+1))\n",
      "localModelScores = np.zeros((trials, size(cluster_sizes)))\n",
      "meanModelScores = np.zeros((trials, size(cluster_sizes)))\n",
      "modelVariance = np.zeros((trials, size(cluster_sizes)))\n",
      "\n",
      "\n",
      "for t in range(0, trials):\n",
      "    print \"Starting trial: \" + str(t)\n",
      "    # Randomly create training and test split\n",
      "    shuffle(indices)\n",
      "    Xtest = X[indices[:testSize],:]\n",
      "    Ytest = Y[indices[:testSize]]\n",
      "    Xtrain = X[indices[testSize:],:]\n",
      "    Ytrain = Y[indices[testSize:]]\n",
      "\n",
      "    # Train a global model on all the data\n",
      "    globalModel = linear_model.LogisticRegression()\n",
      "    globalModel.fit(Xtrain, Ytrain)\n",
      "    globalModelScores[t] = globalModel.score(Xtest, Ytest)\n",
      "    globalModels[t,:] = np.append(globalModel.coef_.flatten(), globalModel.intercept_)\n",
      "    \n",
      "    # Train local models on each machine\n",
      "    for m in range(0, size(cluster_sizes)):\n",
      "        num_machines = cluster_sizes[m]\n",
      "        split_Xtrain = np.array_split(Xtrain, num_machines)\n",
      "        split_Ytrain = np.array_split(Ytrain, num_machines)\n",
      "        # Allocate arrays to store local model info\n",
      "        coeffs = np.zeros((num_machines, p+1))\n",
      "        scores = np.zeros(num_machines)\n",
      "        \n",
      "        # Train local models\n",
      "        for i in range(0, num_machines):\n",
      "            localModel = linear_model.LogisticRegression()\n",
      "            localModel.fit(split_Xtrain[i], split_Ytrain[i])\n",
      "            scores[i] = localModel.score(Xtest, Ytest)\n",
      "            coeffs[i, :] = np.append(localModel.coef_.flatten(), localModel.intercept_)\n",
      "        \n",
      "        # Compute cluster statistics\n",
      "        localModelScores[t,m] = np.mean(scores)\n",
      "        modelVariance[t,m] = np.mean(np.square(coeffs - globalModels[t,:]))\n",
      "        \n",
      "        # Super hack to try and estimate the score of the model obtained by averaging the models\n",
      "        avgCoeff = np.array([np.mean(coeffs, axis=0)])\n",
      "        meanModel = linear_model.LogisticRegression()\n",
      "        # Need to first train the model to set extra class variables\n",
      "        meanModel.fit(split_Xtrain[0], split_Ytrain[0])\n",
      "        # swap out the model parameters with mean parameters\n",
      "        meanModel.coef_ = avgCoeff[:, :-1]\n",
      "        meanModel.intercept_ = avgCoeff[:,-1]\n",
      "        meanModelScores[t,m] = meanModel.score(Xtest, Ytest)\n",
      "\n",
      "        \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean((Y +2.0)/2.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Global Bias: \" + str(np.mean((Y +2.0)/2.0))\n",
      "print \"Global Accuracy: \" + str( np.mean(globalModelScores))\n",
      "plot(cluster_sizes, np.mean(localModelScores, axis=0) / np.mean(globalModelScores))\n",
      "plot(cluster_sizes, np.mean(meanModelScores, axis=0) / np.mean(globalModelScores))\n",
      "legend([\"local\", \"Mean Model\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(np.mean(localModelScores, axis=0))\n",
      "plot(np.mean(meanModelScores, axis=0))\n",
      "legend([\"local\", \"Mean Model\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}